{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "daf0462d-f165-4695-ae8b-7e901f2094ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scripts import *\n",
    "import torch\n",
    "from ipywidgets import HBox, Label, VBox"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6225adf9-fea2-4ed5-a8d4-9e580d915b3f",
   "metadata": {},
   "source": [
    "# Training the Wave U Net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f925422f-d43a-482d-8117-25e2bae73618",
   "metadata": {},
   "source": [
    "## Preparing Training DataLoader and Testing DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2abe60-d4ba-4d20-a234-d213207fd373",
   "metadata": {},
   "source": [
    "We point to the dataset we just made in `CreateDataset.ipynb` and create a Dataset object, which, when indexed with an integer, returns a sample tuple of the form `(mixture_audio, seperated_stems)`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8bf9a0e-603b-4828-80f5-ed09b5efd807",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_folder = \"./data\"\n",
    "hdf_dir_train = f\"{data_folder}/training_data.h5\"\n",
    "hdf_dir_test = f\"{data_folder}/testing_data.h5\"\n",
    "\n",
    "SSDTrain = SourceSeperationDataset(hdf_dir_train)\n",
    "SSDTest = SourceSeperationDataset(hdf_dir_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2a0994-6f92-4ab6-82f9-1e6810c34dd0",
   "metadata": {},
   "source": [
    "We then load that dataset object into a pytorch Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26c6b782-7702-4987-b876-d76daaa2d601",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "DatasetTrainLoader = DataLoader(SSDTrain, batch_size=16, shuffle=True)\n",
    "DatasetTestLoader = DataLoader(SSDTest, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26877cf3-cd06-45e4-bd34-edd262b3f778",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create the WaveUNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a700849-3246-4f66-b96d-d96991a5daa1",
   "metadata": {},
   "source": [
    "We will try to run this on the GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "366ec32d-0c83-4510-b9ab-158cb462202a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f7bafd-2f18-41a9-8969-8ad885891ef5",
   "metadata": {},
   "source": [
    "Lets define a WaveUNet with:\n",
    "\n",
    "- 12 Layers\n",
    "- 24 additional filters per layer\n",
    "- 1 input channel (because theres a mono soundfile)\n",
    "- 4 output channels (because we're seperating into 4 instruments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad4d749a-d75c-40c9-90d7-dec725a3f904",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = WaveUNet(L=12,Fc=24,in_channels=1,out_channels=4)\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee91b313-7cd1-4eb3-ae24-49907481a854",
   "metadata": {},
   "source": [
    "## Pull model from saved one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c75bb0e0-ba9e-40ce-b6ac-27e94ece48c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_folder = \"./models\"\n",
    "model_file = f\"{model_folder}/WaveUNet_Full_Kevin.model\"\n",
    "model.load_state_dict(torch.load(model_file)());"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd3bfe4-6313-4238-a9a1-07469eee1f91",
   "metadata": {},
   "source": [
    "# Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "5250ddd0-f1df-4b9a-a55e-3ad9cafa9b88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import librosa\n",
    "y, sr = librosa.load('./testing-songs/', sr=44100//2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "b2c3c542-9b43-4c3c-9558-032f2dc67246",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import Audio, display\n",
    "from ipywidgets import widgets\n",
    "\n",
    "AS = AudioSection(torch.tensor(y.reshape((1,*y.shape))), sr)\n",
    "audio_sections = AS.cut_into_sections_based_on_samples(2**14)\n",
    "pieces = audio_sections[0:100]\n",
    "with torch.no_grad():\n",
    "    new_pieces = [model(p.audio.view((1,*p.audio.shape)).to(device)) for p in pieces]\n",
    "    new = torch.cat(new_pieces, dim=-1)\n",
    "    new = new.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "adf64435-9485-4215-9c82-5a60978da531",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0556e3ae4014426999bf3abd5f5dbd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Label(value='Original: '), Output())), HBox(children=(Label(value='Drums: '), Ouâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_audio_out(audio):\n",
    "    out = widgets.Output()\n",
    "    with out:\n",
    "        display(audio)\n",
    "    return out\n",
    "\n",
    "def audio_torch(tens):\n",
    "    return get_audio_out(Audio(tens.cpu().numpy(), rate=44100//2))\n",
    "\n",
    "with torch.no_grad():\n",
    "    drums = HBox([Label(\"Drums: \"), get_audio_out(Audio(new[0,0,:].cpu().numpy(), rate=44100//2))])\n",
    "    bass = HBox([Label(\"Bass: \"), get_audio_out(Audio(new[0,1,:].cpu().numpy(), rate=44100//2))])\n",
    "    other = HBox([Label(\"Other: \"), get_audio_out(Audio(new[0,2,:].cpu().numpy(), rate=44100//2))])\n",
    "    vocals = HBox([Label(\"Vocals: \"), get_audio_out(Audio(new[0,3,:].cpu().numpy(), rate=44100//2))])\n",
    "    original = HBox([Label(\"Original: \"), get_audio_out(Audio(AS.audio[0,0:(100*2**14)].cpu().numpy(), rate=44100//2))])\n",
    "    display(VBox([original, drums, bass, other, vocals]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80af856-dc18-404d-a7ae-20ef6ef68dd6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886825b1-1891-44d2-b3b1-3e59fa3701ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3556584-339e-4e13-84ee-7cd2e410702c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
