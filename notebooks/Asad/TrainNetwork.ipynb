{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "919d6f4d-f614-436b-91bc-a4fec1e6ac10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scripts import *\n",
    "from tqdm.notebook import tqdm\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be99d63a-705c-4ad9-aabe-3406e00e3bc3",
   "metadata": {},
   "source": [
    "# Training the Wave U Net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21d187e-0712-4c5f-a59a-4583672d4312",
   "metadata": {},
   "source": [
    "## Preparing Training DataLoader and Testing DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d43824-55f1-495c-8cd1-67549b30625b",
   "metadata": {},
   "source": [
    "We point to the dataset we just made in `CreateDataset.ipynb` and create a Dataset object, which, when indexed with an integer, returns a sample tuple of the form `(mixture_audio, seperated_stems)`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bba749d6-9d3a-4b2b-bf9f-a05a2cff7cb7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_folder = \"./data\"\n",
    "hdf_dir_train = f\"{data_folder}/training_data_stereo.h5\"\n",
    "hdf_dir_test = f\"{data_folder}/testing_data_stereo.h5\"\n",
    "\n",
    "SSDTrain = SourceSeperationDataset(hdf_dir_train)\n",
    "SSDTest = SourceSeperationDataset(hdf_dir_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abcffb3-ca27-46c8-8e15-a4e42d748875",
   "metadata": {},
   "source": [
    "We then load that dataset object into a pytorch Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fa2632f-4fb5-4e5a-ad6e-d07cf867f3fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "DatasetTrainLoader = DataLoader(SSDTrain, batch_size=16, shuffle=True)\n",
    "DatasetTestLoader = DataLoader(SSDTest, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014730b3-55f3-4b9e-9466-8ec40a02f075",
   "metadata": {},
   "source": [
    "we can now iterate through the dataloaders, which will return for us minibatches of tensors. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c777f71-2733-4751-bd99-5eb7bea96c98",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 1, 2, 16384]), torch.Size([16, 4, 2, 16384]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(lambda x: (x[0].shape, x[1].shape))(next(iter(DatasetTrainLoader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7a3e13-bf20-4ffd-9cbf-0deaba3454bd",
   "metadata": {},
   "source": [
    "Note that the shape of the input tensors and output tensors are:\n",
    "\n",
    "`(batch_size)x(instruments)x(audio_channels)x(audio_samples)`\n",
    "\n",
    "For the input we have:  `(16)x(1)x(1)x(16384)`\n",
    "\n",
    "For the output we have: `(16)x(4)x(1)x(16384)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d05e75a-5459-4146-9c0a-e132c76ddf30",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create the WaveUNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5f2664-cbc4-42f6-902b-7499269ec1aa",
   "metadata": {},
   "source": [
    "We will try to run this on the GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba6c020e-f57b-4f8b-9905-f1b36c0a8b12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595ec434-a89b-4c46-8658-995ac625d576",
   "metadata": {},
   "source": [
    "Lets define a WaveUNet with:\n",
    "\n",
    "- 12 Layers\n",
    "- 24 additional filters per layer\n",
    "- 1 input channel (because theres a mono soundfile)\n",
    "- 4 output channels (because we're seperating into 4 instruments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43499572-4c01-48f0-afc7-bdfd2f44d864",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "WN_kevin_stereo = WaveUNet(L=12,Fc=24,in_channels=2,out_channels=8, mono=False)\n",
    "WN_kevin_stereo.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a4752fb8-2b92-4180-8a33-796b747d7f9c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d97ed240ba846fdacc8c9c04c22d1a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       " outer:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e94b63745224d2c9c96707fad89ac8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1921 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m X \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mto(device);\n\u001b[1;32m     27\u001b[0m Y \u001b[38;5;241m=\u001b[39m Y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 28\u001b[0m Y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m writer\u001b[38;5;241m.\u001b[39madd_audio(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSample Input\u001b[39m\u001b[38;5;124m\"\u001b[39m,   X[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, :] ,sample_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m44100\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     30\u001b[0m writer\u001b[38;5;241m.\u001b[39madd_audio(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSample Vocals\u001b[39m\u001b[38;5;124m\"\u001b[39m,  Y_pred[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :] ,sample_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m44100\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/GitHub/Wave-U-Net-final-project/environments/notebook-venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/GitHub/Wave-U-Net-final-project/scripts/WaveUNet.py:136\u001b[0m, in \u001b[0;36mWaveUNet.forward\u001b[0;34m(self, X0)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mUSPs)):\n\u001b[1;32m    135\u001b[0m     reversed_i \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mUSPs) \u001b[38;5;241m-\u001b[39m i \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 136\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mUSPs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mreversed_i\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDSBs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshorts\u001b[49m\u001b[43m[\u001b[49m\u001b[43mreversed_i\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    137\u001b[0m     Xs\u001b[38;5;241m.\u001b[39mappend(X);   \n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mFL[\u001b[38;5;241m0\u001b[39m](Xs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], X0)\n",
      "File \u001b[0;32m~/Documents/GitHub/Wave-U-Net-final-project/environments/notebook-venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/GitHub/Wave-U-Net-final-project/scripts/WaveUNet.py:41\u001b[0m, in \u001b[0;36mUpsamplingBlock.forward\u001b[0;34m(self, x, shortcut)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, shortcut):\n\u001b[0;32m---> 41\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupsample1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m     combined \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_x_like_y(shortcut, x)], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     43\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(combined)\n",
      "File \u001b[0;32m~/Documents/GitHub/Wave-U-Net-final-project/environments/notebook-venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/GitHub/Wave-U-Net-final-project/environments/notebook-venv/lib/python3.8/site-packages/torch/nn/modules/upsampling.py:156\u001b[0m, in \u001b[0;36mUpsample.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterpolate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_factor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malign_corners\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mrecompute_scale_factor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecompute_scale_factor\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/Wave-U-Net-final-project/environments/notebook-venv/lib/python3.8/site-packages/torch/nn/functional.py:3929\u001b[0m, in \u001b[0;36minterpolate\u001b[0;34m(input, size, scale_factor, mode, align_corners, recompute_scale_factor, antialias)\u001b[0m\n\u001b[1;32m   3926\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnti-alias option is only supported for bilinear and bicubic modes\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   3928\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnearest\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 3929\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupsample_nearest1d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_factors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3930\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m4\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnearest\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   3931\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mupsample_nearest2d(\u001b[38;5;28minput\u001b[39m, output_size, scale_factors)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()\n",
    "\n",
    "learning_rate_table = [1e-4, 1e-4, 1e-4] + [1e-4]*20\n",
    "#optimizer = torch.optim.SGD(WUN.parameters(), lr=learning_rate)\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "model = WN_kevin_stereo  # Set the net you want to train to this model\n",
    "model.to(device)\n",
    "\n",
    "n = 0\n",
    "for epoch in tqdm(range(len(learning_rate_table)), desc=\" outer\"):\n",
    "    learning_rate = learning_rate_table[epoch]\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    for batch, (train_combined, train_seperated) in tqdm(enumerate(DatasetTrainLoader), total=len(DatasetTrainLoader)):\n",
    "        ### Produce a piece of Audio on test set\n",
    "        with torch.no_grad():\n",
    "            test_combined, test_seperated = next(iter(DatasetTestLoader))\n",
    "            (batch_size, instruments_in, audio_channels, audio_samples) = test_combined.shape\n",
    "            X = test_combined.view((batch_size, instruments_in*audio_channels, audio_samples));\n",
    "\n",
    "            (batch_size, instruments_out, audio_channels, audio_samples) = test_seperated.shape\n",
    "            Y = test_seperated.view((batch_size, instruments_out*audio_channels, audio_samples));\n",
    "\n",
    "            X = X.to(device);\n",
    "            Y = Y.to(device)\n",
    "            Y_pred = model(X)\n",
    "            writer.add_audio(\"Sample Input\",   X[0, 0, :] ,sample_rate=44100//2)\n",
    "            writer.add_audio(\"Sample Vocals\",  Y_pred[0, -1, :] ,sample_rate=44100//2)\n",
    "            writer.add_audio(\"Actual Vocals\",  Y[0, -1, :] ,sample_rate=44100//2)\n",
    "            writer.add_audio(\"Sample Drums\",  Y_pred[0, 0, :] ,sample_rate=44100//2)\n",
    "            writer.add_audio(\"Actual Drums\",  Y[0, 0, :] ,sample_rate=44100//2)\n",
    "            writer.add_audio(\"Sample Bass\",  Y_pred[0, 1, :] ,sample_rate=44100//2)\n",
    "            writer.add_audio(\"Actual Bass\",  Y[0, 1, :] ,sample_rate=44100//2)\n",
    "            writer.add_audio(\"Sample Other\",  Y_pred[0, -2, :] ,sample_rate=44100//2)\n",
    "            writer.add_audio(\"Actual Other\",  Y[0, -2, :] ,sample_rate=44100//2)\n",
    "            if (epoch % 2) == 0:\n",
    "                datetime_now = f\"{datetime.datetime.now()}\".replace(\":\", \"-\").split(\".\")[0]\n",
    "                torch.save(model.state_dict, f\"./checkpoints/date{datetime_now}_epoch{epoch}.model\")\n",
    "            \n",
    "        (batch_size, instruments_in, audio_channels, audio_samples) = train_combined.shape\n",
    "        X = train_combined.view((batch_size, instruments_in*audio_channels, audio_samples));\n",
    "\n",
    "        (batch_size, instruments_out, audio_channels, audio_samples) = train_seperated.shape\n",
    "        Y = train_seperated.view((batch_size, instruments_out*audio_channels, audio_samples));\n",
    "\n",
    "        X = X.to(device);\n",
    "        Y = Y.to(device)\n",
    "\n",
    "        #print()\n",
    "        #for j in range(20):\n",
    "        Y_pred = model(X)\n",
    "        loss = criterion(Y_pred, Y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        n += 1\n",
    "\n",
    "        #if batch % 10 == 0:\n",
    "        writer.add_scalar(\"Loss/train\", loss, n)\n",
    "            #loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            #writer.add_scalar(\"Loss/train\", loss, n)\n",
    "            #print(f\"loss: {loss:>7f}\")\n",
    "            \n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92f020e-0348-42ae-90ee-dccd4e14e8ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3be825c2-330a-4f2c-af96-217faf78c3eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "writer.add_audio(\"Sample Other\", Y_pred[0,-2,:].view((1, *Y_pred[0,-2,:].shape)), sample_rate=44100//2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "954155b0-c4a8-4dd0-9c2d-a2df7e097cf5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16384])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred[0,-2,:].view((1, *Y_pred[0,-2,:].shape)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc49bb99-8314-48ca-8e8d-f3330e14261b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
