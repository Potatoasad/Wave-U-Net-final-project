{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "919d6f4d-f614-436b-91bc-a4fec1e6ac10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scripts import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be99d63a-705c-4ad9-aabe-3406e00e3bc3",
   "metadata": {},
   "source": [
    "# Training the Wave U Net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21d187e-0712-4c5f-a59a-4583672d4312",
   "metadata": {},
   "source": [
    "## Preparing Training DataLoader and Testing DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d43824-55f1-495c-8cd1-67549b30625b",
   "metadata": {},
   "source": [
    "We point to the dataset we just made in `CreateDataset.ipynb` and create a Dataset object, which, when indexed with an integer, returns a sample tuple of the form `(mixture_audio, seperated_stems)`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bba749d6-9d3a-4b2b-bf9f-a05a2cff7cb7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_folder = \"./data\"\n",
    "hdf_dir_train = f\"{data_folder}/training_data.h5\"\n",
    "hdf_dir_test = f\"{data_folder}/testing_data.h5\"\n",
    "\n",
    "SSDTrain = SourceSeperationDataset(hdf_dir_train)\n",
    "SSDTest = SourceSeperationDataset(hdf_dir_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abcffb3-ca27-46c8-8e15-a4e42d748875",
   "metadata": {},
   "source": [
    "We then load that dataset object into a pytorch Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fa2632f-4fb5-4e5a-ad6e-d07cf867f3fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "DatasetTrainLoader = DataLoader(SSDTrain, batch_size=16, shuffle=True)\n",
    "DatasetTestLoader = DataLoader(SSDTest, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014730b3-55f3-4b9e-9466-8ec40a02f075",
   "metadata": {},
   "source": [
    "we can now iterate through the dataloaders, which will return for us minibatches of tensors. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c777f71-2733-4751-bd99-5eb7bea96c98",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 1, 1, 16384]), torch.Size([16, 4, 1, 16384]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(lambda x: (x[0].shape, x[1].shape))(next(iter(DatasetTrainLoader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7a3e13-bf20-4ffd-9cbf-0deaba3454bd",
   "metadata": {},
   "source": [
    "Note that the shape of the input tensors and output tensors are:\n",
    "\n",
    "`(batch_size)x(instruments)x(audio_channels)x(audio_samples)`\n",
    "\n",
    "For the input we have:  `(16)x(1)x(1)x(16384)`\n",
    "\n",
    "For the output we have: `(16)x(4)x(1)x(16384)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d05e75a-5459-4146-9c0a-e132c76ddf30",
   "metadata": {},
   "source": [
    "## Create the WaveUNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5f2664-cbc4-42f6-902b-7499269ec1aa",
   "metadata": {},
   "source": [
    "We will try to run this on the GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba6c020e-f57b-4f8b-9905-f1b36c0a8b12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595ec434-a89b-4c46-8658-995ac625d576",
   "metadata": {},
   "source": [
    "Lets define a WaveUNet with:\n",
    "\n",
    "- 12 Layers\n",
    "- 24 additional filters per layer\n",
    "- 1 input channel (because theres a mono soundfile)\n",
    "- 4 output channels (because we're seperating into 4 instruments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43499572-4c01-48f0-afc7-bdfd2f44d864",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "WN_kevin = WaveUNet(L=12,Fc=24,in_channels=1,out_channels=4)\n",
    "WN_kevin.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4752fb8-2b92-4180-8a33-796b747d7f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate_table = [1e-4, 1e-4, 1e-4] + [1e-4]*20\n",
    "#optimizer = torch.optim.SGD(WUN.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "model = WN_kevin  # Set the net you want to train to this model\n",
    "model.to(device)\n",
    "\n",
    "n = 0\n",
    "for epoch in tqdm(range(len(learning_rate_table)), desc=\" outer\"):\n",
    "    learning_rate = learning_rate_table[epoch]\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    for batch, (train_combined, train_seperated) in tqdm(enumerate(DatasetLoader), total=len(DatasetLoader)):\n",
    "        (batch_size, instruments_in, audio_channels, audio_samples) = train_combined.shape\n",
    "        X = train_combined.view((batch_size, instruments_in*audio_channels, audio_samples));\n",
    "\n",
    "        (batch_size, instruments_out, audio_channels, audio_samples) = train_seperated.shape\n",
    "        Y = train_seperated.view((batch_size, instruments_out*audio_channels, audio_samples));\n",
    "\n",
    "        X = X.to(device);\n",
    "        Y = Y.to(device)\n",
    "\n",
    "        #print()\n",
    "        #for j in range(20):\n",
    "        Y_pred = model(X)\n",
    "        loss = criterion(Y_pred, Y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        n += 1\n",
    "\n",
    "        #if batch % 10 == 0:\n",
    "        writer.add_scalar(\"Loss/train\", loss, n)\n",
    "            #loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            #writer.add_scalar(\"Loss/train\", loss, n)\n",
    "            #print(f\"loss: {loss:>7f}\")\n",
    "        \n",
    "    ### Produce a piece of Audio on test set\n",
    "    with torch.no_grad():\n",
    "        test_combined, test_seperated = next(iter(DatasetTestLoader))\n",
    "        (batch_size, instruments_in, audio_channels, audio_samples) = test_combined.shape\n",
    "        X = test_combined.view((batch_size, instruments_in*audio_channels, audio_samples));\n",
    "\n",
    "        (batch_size, instruments_out, audio_channels, audio_samples) = test_seperated.shape\n",
    "        Y = test_seperated.view((batch_size, instruments_out*audio_channels, audio_samples));\n",
    "\n",
    "        X = X.to(device);\n",
    "        Y = Y.to(device)\n",
    "        Y_pred = model(X)\n",
    "        writer.add_audio(\"Sample Input\",   X[0, 0, :] ,sample_rate=44100//2)\n",
    "        writer.add_audio(\"Sample Vocals\",  Y_pred[0, -1, :] ,sample_rate=44100//2)\n",
    "        writer.add_audio(\"Actual Vocals\",  Y[0, -1, :] ,sample_rate=44100//2)\n",
    "        writer.add_audio(\"Sample Drums\",  Y_pred[0, 0, :] ,sample_rate=44100//2)\n",
    "        writer.add_audio(\"Actual Drums\",  Y[0, 0, :] ,sample_rate=44100//2)\n",
    "        writer.add_audio(\"Sample Bass\",  Y_pred[0, 1, :] ,sample_rate=44100//2)\n",
    "        writer.add_audio(\"Actual Bass\",  Y[0, 1, :] ,sample_rate=44100//2)\n",
    "        writer.add_audio(\"Sample Other\",  Y_pred[0, -2, :] ,sample_rate=44100//2)\n",
    "        writer.add_audio(\"Actual Other\",  Y[0, -2, :] ,sample_rate=44100//2)\n",
    "        if (epoch % 2) == 0:\n",
    "            datetime_now = f\"{datetime.datetime.now()}\".replace(\":\", \"-\").split(\".\")[0]\n",
    "            torch.save(model.state_dict, f\"./the_wn1_updated.model_checkpoint_epoch{epoch}\")\n",
    "            \n",
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
