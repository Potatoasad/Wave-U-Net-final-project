{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e7f4a6a-b9a6-4505-b97b-9817c86a8ca0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scripts import *\n",
    "from tqdm.notebook import tqdm\n",
    "import h5py\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde3a41b-16df-4f52-88d8-b6676e138929",
   "metadata": {},
   "source": [
    "# Creating the Dataset\n",
    "\n",
    "## Downloading the files into the right place\n",
    "\n",
    "First we set up the Data Folder which will hold the MUSDB data set. We want to directly download it into the `data/raw` directory next to this notebook. We will make that directory if it's not there. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4ad5adb-52e3-48f1-a09c-fa3c218a56ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "data_folder = \"./data\"\n",
    "raw_dataset_folder = f\"{data_folder}/raw\"\n",
    "\n",
    "##  Create them if they don't exist\n",
    "if not os.path.exists(data_folder):\n",
    "    os.mkdir(data_folder)\n",
    "    if not os.path.exists(raw_dataset_folder):\n",
    "        os.mkdir(raw_dataset_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddee4866-d1b8-4f3d-8026-157d9b887321",
   "metadata": {},
   "source": [
    "Now we download the data into that folder, you only need to run this once of course. This will take a whileeeeee. it's 4.7GB \n",
    "\n",
    "1. __Go to the following url to download the dataset: https://zenodo.org/record/1117372__\n",
    "\n",
    "2. Then unzip the file into `data/raw` such that the file directory looks like:\n",
    "```\n",
    "├── Asad\n",
    "|   ├── NotebookSkeleton.ipynb\n",
    "│   ├── data\n",
    "│   │   ├── raw\n",
    "│   │   │   ├── test\n",
    "│   │   │   ├── train\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415eec59-0b64-410d-a900-9c0debeef4d5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Now we can create the dataset\n",
    "\n",
    "We use the musdb library to get a nice API to query for audio files stored within the `mp4` files in the raw dataset.\n",
    "\n",
    "We set the root of the dataset to be the `./data/raw` folder we created earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "080539ec-c1af-4a3d-aafd-786a219cb571",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import musdb\n",
    "\n",
    "mus = musdb.DB(root=raw_dataset_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ef655b-5191-445b-99ba-112db8adbe82",
   "metadata": {},
   "source": [
    "We get our data split into training data and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16223fca-4e13-4fd7-bfa3-3d3992e147c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_data = mus.load_mus_tracks(\"train\")\n",
    "testing_data = mus.load_mus_tracks(\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369c24e5-453c-4eaf-a4e8-53e85d0a6ea3",
   "metadata": {},
   "source": [
    "Now we make a dataset. \n",
    "\n",
    "1. We know that our Neural Network will take in data with $2^{14} = 16384$ samples. \n",
    "2. We would like our dataset to be made using samples that are converted to mono\n",
    "3. We would like to use a smaller sample rate so that 16384 samples cover a reasonable amount of time. So we will choose to downsample the music such that the sample rate is reduced by half `downsampling_ratio=2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f8a08bd-89c1-4bdb-9b4f-42e8ac147d42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Downsampling ratio \n",
    "downsampling_ratio = 2    \n",
    "\n",
    "# Samples inside one segment of data\n",
    "samples_per_segment = 2**14 # = 16384\n",
    "\n",
    "# Convert to mono\n",
    "convert_to_mono = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e8d0c4-02e4-4997-9015-b00ddcb9908f",
   "metadata": {},
   "source": [
    "### Create training data set as an HDF5 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0148e2f1-946e-4cb7-8b5c-116e05cedbb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "643fa8df0ab04e50a636a1163b17e832",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating & Saving Dataset:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "song_db = []\n",
    "idx = 0\n",
    "hdf_dir = f\"{data_folder}/training_data.h5\"\n",
    "\n",
    "for track in tqdm(training_data,  desc=\"Creating & Saving Dataset\", position=0):\n",
    "    # downsample data\n",
    "    stems = Track(track).get_stem_section(ds=downsampling_ratio) \n",
    "    \n",
    "    # Convert to mono\n",
    "    if convert_to_mono:\n",
    "        stems.convert_to_mono()\n",
    "        \n",
    "    # Cut the samples into segements to be analysed. The neural net architecture should take this many samples \n",
    "    # as inputs\n",
    "    thebrokenstems = stems.cut_into_sections_based_on_samples(samples_per_segment) # Cut it into segments of samples_per_segment each\n",
    "    \n",
    "    with h5py.File(hdf_dir, \"a\") as f:\n",
    "        instruments = [\"drums\", \"bass\", \"other\", \"vocals\"]\n",
    "        f.attrs[\"samplerate\"] = thebrokenstems[0].rate\n",
    "        f.attrs[\"channels\"] = 1 if thebrokenstems[0].is_mono else 2\n",
    "        f.attrs[\"instruments\"] = instruments\n",
    "        \n",
    "        for example in thebrokenstems:\n",
    "            source_audios = example.get_stacked_audio(instruments)\n",
    "            mix_audio = torch.sum(source_audios, dim=0)\n",
    "            mix_audio = mix_audio.reshape((1, *mix_audio.shape))\n",
    "\n",
    "            source_audios = source_audios.numpy()\n",
    "            mix_audio = mix_audio.numpy()\n",
    "\n",
    "            # Add to HDF5 file\n",
    "            grp = f.create_group(str(idx))\n",
    "            grp.create_dataset(\"inputs\", shape=mix_audio.shape, dtype=mix_audio.dtype, data=mix_audio)\n",
    "            grp.create_dataset(\"targets\", shape=source_audios.shape, dtype=source_audios.dtype, data=source_audios)\n",
    "\n",
    "            grp.attrs[\"length\"] = mix_audio.shape[1]\n",
    "            grp.attrs[\"target_length\"] = source_audios.shape[1]\n",
    "            song_db.append({\"idx\" : idx, \"name\" : track.name, \"artist\" : track.artist, \"duration\" : track.duration})\n",
    "            idx += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d11780e-1068-4ed9-b692-34e669c70d4f",
   "metadata": {},
   "source": [
    "### Creating testing dataset as HDF5 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f257b6a5-807f-48b3-a533-36cb3f71aedf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e397afc441b64ff1a58b9f98fb1373e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating & Saving Dataset:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Unable to create group (name already exists)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 32\u001b[0m\n\u001b[1;32m     29\u001b[0m mix_audio \u001b[38;5;241m=\u001b[39m mix_audio\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Add to HDF5 file\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m grp \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_group\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m grp\u001b[38;5;241m.\u001b[39mcreate_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs\u001b[39m\u001b[38;5;124m\"\u001b[39m, shape\u001b[38;5;241m=\u001b[39mmix_audio\u001b[38;5;241m.\u001b[39mshape, dtype\u001b[38;5;241m=\u001b[39mmix_audio\u001b[38;5;241m.\u001b[39mdtype, data\u001b[38;5;241m=\u001b[39mmix_audio)\n\u001b[1;32m     34\u001b[0m grp\u001b[38;5;241m.\u001b[39mcreate_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtargets\u001b[39m\u001b[38;5;124m\"\u001b[39m, shape\u001b[38;5;241m=\u001b[39msource_audios\u001b[38;5;241m.\u001b[39mshape, dtype\u001b[38;5;241m=\u001b[39msource_audios\u001b[38;5;241m.\u001b[39mdtype, data\u001b[38;5;241m=\u001b[39msource_audios)\n",
      "File \u001b[0;32m~/Documents/GitHub/Wave-U-Net-final-project/environments/notebook-venv/lib/python3.8/site-packages/h5py/_hl/group.py:64\u001b[0m, in \u001b[0;36mGroup.create_group\u001b[0;34m(self, name, track_order)\u001b[0m\n\u001b[1;32m     62\u001b[0m name, lcpl \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_e(name, lcpl\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     63\u001b[0m gcpl \u001b[38;5;241m=\u001b[39m Group\u001b[38;5;241m.\u001b[39m_gcpl_crt_order \u001b[38;5;28;01mif\u001b[39;00m track_order \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m gid \u001b[38;5;241m=\u001b[39m \u001b[43mh5g\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlcpl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlcpl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgcpl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgcpl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Group(gid)\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/h5g.pyx:166\u001b[0m, in \u001b[0;36mh5py.h5g.create\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Unable to create group (name already exists)"
     ]
    }
   ],
   "source": [
    "song_db_test = []\n",
    "idx = 0\n",
    "hdf_dir_test = f\"{data_folder}/testing_data.h5\"\n",
    "\n",
    "for track in tqdm(testing_data, desc=\"Creating & Saving Dataset\", position=0):\n",
    "    # downsample data\n",
    "    stems = Track(track).get_stem_section(ds=downsampling_ratio) \n",
    "    \n",
    "    # Convert to mono\n",
    "    if convert_to_mono:\n",
    "        stems.convert_to_mono()\n",
    "        \n",
    "    # Cut the samples into segements to be analysed. The neural net architecture should take this many samples \n",
    "    # as inputs\n",
    "    thebrokenstems = stems.cut_into_sections_based_on_samples(samples_per_segment) # Cut it into segments\n",
    "    \n",
    "    with h5py.File(hdf_dir_test, \"a\") as f:\n",
    "        instruments = [\"drums\", \"bass\", \"other\", \"vocals\"]\n",
    "        f.attrs[\"samplerate\"] = thebrokenstems[0].rate\n",
    "        f.attrs[\"channels\"] = 1 if thebrokenstems[0].is_mono else 2\n",
    "        f.attrs[\"instruments\"] = instruments\n",
    "        \n",
    "        for example in thebrokenstems:\n",
    "            source_audios = example.get_stacked_audio(instruments)\n",
    "            mix_audio = torch.sum(source_audios, dim=0)\n",
    "            mix_audio = mix_audio.reshape((1, *mix_audio.shape))\n",
    "\n",
    "            source_audios = source_audios.numpy()\n",
    "            mix_audio = mix_audio.numpy()\n",
    "\n",
    "            # Add to HDF5 file\n",
    "            grp = f.create_group(str(idx))\n",
    "            grp.create_dataset(\"inputs\", shape=mix_audio.shape, dtype=mix_audio.dtype, data=mix_audio)\n",
    "            grp.create_dataset(\"targets\", shape=source_audios.shape, dtype=source_audios.dtype, data=source_audios)\n",
    "\n",
    "            grp.attrs[\"length\"] = mix_audio.shape[1]\n",
    "            grp.attrs[\"target_length\"] = source_audios.shape[1]\n",
    "            song_db_test.append({\"idx\" : idx, \"name\" : track.name, \"artist\" : track.artist, \"duration\" : track.duration})\n",
    "            idx += 1\n",
    "            \n",
    "#song_db_test = pd.DataFrame(song_db_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285b152d-137f-4e8e-b277-cf823e43a6cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
